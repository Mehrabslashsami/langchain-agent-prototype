{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain wikipedia transformers\n"
      ],
      "metadata": {
        "id": "Nq3oXynjhn3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from transformers import pipeline\n",
        "\n",
        "hf_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", max_new_tokens=128)\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "wiki = WikipediaAPIWrapper()\n"
      ],
      "metadata": {
        "id": "lpZW6kiAhpbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wiki_search(query: str) -> str:\n",
        "    try:\n",
        "        result = wiki.run(query)\n",
        "        if not result.strip():\n",
        "            return \"No relevant info found on Wikipedia.\"\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching Wikipedia data: {e}\"\n"
      ],
      "metadata": {
        "id": "o7HBSltshstJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize this text concisely:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        "\n",
        "def summarize_text(text: str) -> str:\n",
        "    try:\n",
        "        return summary_chain.invoke({\"text\": text})[\"text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Could not summarize due to error: {e}\"\n"
      ],
      "metadata": {
        "id": "MIXo-4hGhv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize this text concisely:\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        "\n",
        "def summarize_text(text: str) -> str:\n",
        "    try:\n",
        "        return summary_chain.invoke({\"text\": text})[\"text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Could not summarize due to error: {e}\"\n"
      ],
      "metadata": {
        "id": "d7jlR0WZhxkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import wrap\n",
        "\n",
        "def chunk_text(text, max_chars=1500):\n",
        "    return wrap(text, max_chars)\n",
        "\n",
        "query = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "# Step 1: Search Wikipedia\n",
        "wiki_result = wiki_search(query)\n",
        "print(\"Wikipedia Result (preview):\\n\", wiki_result[:500], \"...\\n\")  # preview first 500 chars\n",
        "\n",
        "# Step 2: If text is short, summarize directly, else chunk & summarize each\n",
        "if len(wiki_result) < 1500:\n",
        "    final_answer = summarize_text(wiki_result)\n",
        "else:\n",
        "    chunks = chunk_text(wiki_result, max_chars=1500)[:2]  # limit to first 2 chunks for speed\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "        summaries.append(summarize_text(chunk))\n",
        "    final_answer = \" \".join(summaries)\n",
        "\n",
        "print(\"\\nFinal Answer:\\n\", final_answer)\n"
      ],
      "metadata": {
        "id": "mLEFFe5bhzZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n",
        "\n",
        "def chat_agent(query):\n",
        "    wiki_result = wiki_search(query)\n",
        "    if len(wiki_result) < 1500:\n",
        "        return summarize_text(wiki_result)\n",
        "    else:\n",
        "        chunks = chunk_text(wiki_result, max_chars=1500)[:2]\n",
        "        summaries = [summarize_text(chunk) for chunk in chunks]\n",
        "        return \" \".join(summaries)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_agent,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"LangChain Agent Prototype\",\n",
        "    description=\"Ask questions, get answers from Wikipedia + summarization\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "DZxl-WA4h9cg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}